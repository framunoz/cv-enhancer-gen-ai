{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress warning from google-genai when accessing text on function calls\n",
    "warnings.filterwarnings(\"ignore\", message=\"there are non-text parts in the response\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from schemas import JsonResume\n",
    "\n",
    "type FileType = str | Path | None\n",
    "\n",
    "DEFAULT_FILE = Path(\"../data/json_resume_example.json\")\n",
    "\n",
    "\n",
    "def get_json_resume() -> JsonResume:\n",
    "    \"\"\"Load a JSON resume from a file and validate it against the JsonResume schema.\n",
    "\n",
    "    Returns:\n",
    "        JsonResume: The validated JSON resume object.\n",
    "    \"\"\"\n",
    "    with Path(DEFAULT_FILE).open(encoding=\"utf-8\") as json_resume_path:\n",
    "        json_resume = JsonResume.model_validate_json(json_resume_path.read())\n",
    "\n",
    "    return json_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "json_resume = get_json_resume()\n",
    "\n",
    "rprint(json_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "\n",
    "def iterate_bfs(tree: dict) -> t.Generator[tuple[str, dict[str, t.Any]]]:\n",
    "    \"\"\"Iterate through a nested dictionary using BFS (Breadth-First Search).\n",
    "\n",
    "    Args:\n",
    "        tree (dict): The nested dictionary to traverse.\n",
    "\n",
    "    Yields:\n",
    "        tuple[str, dict]: A tuple containing the key and dictionary node for\n",
    "            each dictionary found in the tree structure.\n",
    "    \"\"\"\n",
    "    from collections import deque  # noqa: PLC0415\n",
    "\n",
    "    queue: deque[tuple[str, dict | list]] = deque([(\"root\", tree)])\n",
    "\n",
    "    while queue:\n",
    "        key, current_node = queue.pop()\n",
    "\n",
    "        # Skip non-dict and non-list nodes\n",
    "        if isinstance(current_node, dict):\n",
    "            yield key, current_node\n",
    "\n",
    "        # If the current node is a dictionary, we add its values to the queue\n",
    "        #  (they are the child nodes)\n",
    "        if isinstance(current_node, dict):\n",
    "            for child_key, child_value in current_node.items():\n",
    "                if isinstance(child_value, (dict, list)):\n",
    "                    queue.appendleft((child_key, child_value))\n",
    "\n",
    "        # The node can also be a list. We need to handle that case too.\n",
    "        if isinstance(current_node, list):\n",
    "            for item in current_node:\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    queue.appendleft((key, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "\n",
    "def get_stacks_by_experience() -> dict[str, list[str]]:\n",
    "    \"\"\"Get technology stacks organized by experience type from the JSON resume.\n",
    "\n",
    "    This function traverses the JSON resume structure using BFS (Breadth-First Search)\n",
    "    to find all \"keywords\" fields and organizes them by their parent context\n",
    "    (e.g., work experience, skills, projects, etc.).\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: A dictionary where keys represent the experience\n",
    "            type or context (e.g., 'work', 'skills', 'projects') and values\n",
    "            are sorted lists of unique technology stacks/keywords found in\n",
    "            that context.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict  # noqa: PLC0415\n",
    "\n",
    "    json_resume = get_json_resume()\n",
    "\n",
    "    # Convert the Pydantic model to a dictionary\n",
    "    dict_resume = json_resume.model_dump()\n",
    "\n",
    "    # Iterate through the JSON structure using BFS\n",
    "    stacks: defaultdict[str, list[str]] = defaultdict(list)\n",
    "    for name, experience_node in iterate_bfs(dict_resume):\n",
    "        if keywords := experience_node.get(\"keywords\"):\n",
    "            # Sometimes keywords can be None. We skip those cases.\n",
    "            if keywords is None:\n",
    "                continue\n",
    "            stacks[name].extend(keywords)\n",
    "\n",
    "    # Remove duplicates and sort the stacks\n",
    "    stacks_sorted = {key: sorted(set(value)) for key, value in stacks.items()}\n",
    "\n",
    "    return stacks_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stacks() -> list[str]:\n",
    "    \"\"\"Get a sorted list of all unique technology stacks/keywords from the JSON resume.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the status and a semicolon-separated string\n",
    "            of all unique technology stacks/keywords.\n",
    "    \"\"\"\n",
    "    stacks_by_experience = get_stacks_by_experience()\n",
    "    all_stacks = set()\n",
    "    for stacks in stacks_by_experience.values():\n",
    "        all_stacks.update(stacks)\n",
    "    return sorted(all_stacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We seek to obtain some metric of similarity between experiences and skill stacks. To do this, we will use embeddings generated by a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display as ip_display\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Formatable(t.Protocol):\n",
    "    def format(self) -> str: ...\n",
    "\n",
    "\n",
    "class Identifiable(t.Protocol):\n",
    "    def get_id(self) -> str: ...\n",
    "\n",
    "\n",
    "class WithType(t.Protocol):\n",
    "    @property\n",
    "    def item_type(self) -> str: ...\n",
    "\n",
    "\n",
    "class Dumpable(t.Protocol):\n",
    "    def model_dump(self) -> dict[str, t.Any]: ...\n",
    "\n",
    "\n",
    "class Persistable(Formatable, Identifiable, WithType, Dumpable, t.Protocol):\n",
    "    \"\"\"Protocol for objects that can be formatted and identified for persistence.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def iterate_over_formatables(\n",
    "    json_resume: JsonResume,\n",
    ") -> t.Iterator[Persistable]:\n",
    "    if works := json_resume.work:\n",
    "        yield from works\n",
    "\n",
    "    if volunteers := json_resume.volunteer:\n",
    "        yield from volunteers\n",
    "\n",
    "    if certificates := json_resume.certificates:\n",
    "        yield from certificates\n",
    "\n",
    "    if projects := json_resume.projects:\n",
    "        yield from projects\n",
    "\n",
    "    if skills := json_resume.skills:\n",
    "        yield from skills\n",
    "\n",
    "    if interests := json_resume.interests:\n",
    "        yield from interests\n",
    "\n",
    "\n",
    "experencies: list[Persistable] = []\n",
    "for exp in iterate_over_formatables(json_resume):\n",
    "    experencies.append(exp)\n",
    "    ip_display(Markdown(exp.format()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_experience_from_id(\n",
    "    experience_id: str,\n",
    ") -> Persistable | None:\n",
    "    \"\"\"Find an experience by its ID in the JSON resume.\n",
    "\n",
    "    Args:\n",
    "        experience_id (str): The ID of the experience to find.\n",
    "    Returns:\n",
    "        Persistable | None: The experience object if found, otherwise None.\n",
    "    \"\"\"\n",
    "    json_resume = get_json_resume()\n",
    "    for exp in iterate_over_formatables(json_resume):\n",
    "        if exp.get_id() == experience_id:\n",
    "            return exp\n",
    "    return None\n",
    "\n",
    "\n",
    "find_experience_from_id(\"interest.culture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "for m in client.models.list():\n",
    "    supp_actions = m.supported_actions\n",
    "    if supp_actions and \"embedContent\" in supp_actions:\n",
    "        print(m.name)  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "from google.genai import types\n",
    "from google.genai.errors import APIError\n",
    "\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "def is_retriable(e):\n",
    "    return isinstance(e, APIError) and e.code in {429, 503}\n",
    "\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "    OUTPUT_DIM = 768\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        document_mode: bool = True,\n",
    "        embedding_model: str | None = None,\n",
    "        output_dim: int | None = None,\n",
    "    ):\n",
    "        self.document_mode = document_mode\n",
    "        self.embedding_model = embedding_model\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    @property\n",
    "    def task_type(self) -> str:\n",
    "        return \"RETRIEVAL_DOCUMENT\" if self.document_mode else \"RETRIEVAL_QUERY\"\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    @t.override\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        response = client.models.embed_content(\n",
    "            model=self.embedding_model or self.EMBEDDING_MODEL,\n",
    "            contents=input,  # type: ignore\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=self.task_type,\n",
    "                output_dimensionality=self.output_dim or self.OUTPUT_DIM,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "DB_NAME = \"cv_embeddings_v2\"\n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction(document_mode=True)\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "\n",
    "db.add(\n",
    "    documents=[e.format() for e in experencies],\n",
    "    ids=[e.get_id() for e in experencies],\n",
    "    metadatas=[{\"item_type\": e.item_type} for e in experencies],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to query mode when generating embeddings.\n",
    "embed_fn.document_mode = False\n",
    "\n",
    "# Search the Chroma DB using the specified query.\n",
    "query = \", \".join([\n",
    "    \"Python\",\n",
    "    \"SQL\",\n",
    "    \"Machine Learning\",\n",
    "    \"Data Mining\",\n",
    "    \"Computer Vision\",\n",
    "    \"GCP\",\n",
    "    \"AWS\",\n",
    "    \"Azure\",\n",
    "    \"CI/CD pipelines\",\n",
    "    \"Docker\",\n",
    "    \"OpenCV\",\n",
    "    \"PyTorch\",\n",
    "    \"TensorFlow\",\n",
    "    \"YOLO\",\n",
    "    \"Detectron2\",\n",
    "])\n",
    "query = \", \".join([\n",
    "    \"Python\",\n",
    "    \"SQL\",\n",
    "    \"Machine Learning\",\n",
    "])\n",
    "\n",
    "result = db.query(query_texts=[query], n_results=10)\n",
    "[all_passages] = result[\"documents\"]  # type: ignore\n",
    "\n",
    "Markdown(all_passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.base_types import Metadata\n",
    "\n",
    "\n",
    "class ExperienceRetrieval(t.TypedDict):\n",
    "    exp_id: str\n",
    "    exp_type: str\n",
    "    description: str\n",
    "    distance: float\n",
    "\n",
    "\n",
    "class ExperienceRetrievalResult(t.TypedDict, total=False):\n",
    "    status: str\n",
    "    retrieved_experiences: list[ExperienceRetrieval]\n",
    "    message: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb084bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def retrive_experiences_by_query(\n",
    "    query: str,\n",
    "    n_results: int,\n",
    ") -> ExperienceRetrievalResult:\n",
    "    \"\"\"Retrieve experiences from the ChromaDB based on a query string.\n",
    "    The embedding function will be switched to query mode during this operation.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for.\n",
    "        n_results (int): The number of top results to retrieve. If the value\n",
    "            is less than or equal to zero, all experiences will be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the status and retrieved experiences.\n",
    "\n",
    "    Example:\n",
    "        >>> retrive_experiences_by_query(\n",
    "        ...     query=\"Python, SQL, Machine Learning\",\n",
    "        ...     n_results=2,\n",
    "        ... )\n",
    "        {\n",
    "            \"status\": \"success\",\n",
    "            \"retrieved_experiences\": \"[\\n  {\\n    \\\"exp_id\\\": \\\"work_1\\\",\\n    \\\"description\\\": \\\"Developed data pipelines using Python and SQL...\\\",\\n    \\\"exp_type\\\": \\\"work\\\",\\n    \\\"distance\\\": 0.12345\\n  },\\n  {\\n    \\\"exp_id\\\": \\\"project_3\\\",\\n    \\\"description\\\": \\\"Implemented machine learning models using Python...\\\",\\n    \\\"exp_type\\\": \\\"project\\\",\\n    \\\"distance\\\": 0.23456\\n  }\\n]\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Switch to query mode when generating embeddings.\n",
    "    embed_fn.document_mode = False\n",
    "\n",
    "    # Search the Chroma DB using the specified query.\n",
    "    if n_results <= 0:\n",
    "        n_results = db.count()\n",
    "\n",
    "    try:\n",
    "        result = db.query(query_texts=[query], n_results=n_results)\n",
    "\n",
    "        documents: list[str] = []\n",
    "        ids: list[str] = []\n",
    "        metadatas: list[Metadata] = []\n",
    "        distances: list[float] = []\n",
    "\n",
    "        if result[\"documents\"]:\n",
    "            documents = result[\"documents\"][0]\n",
    "\n",
    "        if result[\"ids\"]:\n",
    "            ids = result[\"ids\"][0]\n",
    "\n",
    "        if result[\"metadatas\"]:\n",
    "            metadatas = result[\"metadatas\"][0]\n",
    "\n",
    "        if result[\"distances\"]:\n",
    "            distances = result[\"distances\"][0]\n",
    "\n",
    "        retrived_experiences = [\n",
    "            ExperienceRetrieval(\n",
    "                exp_type=str(metadata[\"item_type\"]),\n",
    "                description=passage,\n",
    "                exp_id=exp_id,\n",
    "                distance=distance,\n",
    "            )\n",
    "            for exp_id, passage, metadata, distance in zip(\n",
    "                ids, documents, metadatas, distances, strict=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return {\"status\": \"success\", \"retrieved_experiences\": retrived_experiences}\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "retrived_exps = retrive_experiences_by_query(\n",
    "    query=(\n",
    "        \"Python, SQL, Machine Learning, Data Mining, Computer Vision, GCP, AWS,\"\n",
    "        \" Azure, CI/CD pipelines, Docker, OpenCV, PyTorch, TensorFlow, YOLO,\"\n",
    "        \" Detectron2\"\n",
    "    ),\n",
    "    n_results=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# CV Enhancer\n",
    "\n",
    "El objetivo es crear un agente que ayude a mejorar un currículum vitae (CV) agregando y enfocando las experiencias laborales y habilidades relevantes para un puesto de trabajo específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from google.adk.models.google_llm import Gemini\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=10,  # Maximum retry attempts\n",
    "    initial_delay=1,\n",
    "    max_delay=60,\n",
    "    exp_base=3,  # Delay multiplier\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CvSaverConfig:\n",
    "    job_offer_summarizer_model: Gemini\n",
    "    experience_enhancement_model: Gemini\n",
    "    critic_model: Gemini\n",
    "    cv_enhancement_model: Gemini\n",
    "\n",
    "\n",
    "config = CvSaverConfig(\n",
    "    job_offer_summarizer_model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config,\n",
    "    ),\n",
    "    experience_enhancement_model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config,\n",
    "    ),\n",
    "    critic_model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config,\n",
    "    ),\n",
    "    cv_enhancement_model=Gemini(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retry_options=retry_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "rprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac92533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent, BaseAgent\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "from google.adk.events import Event\n",
    "\n",
    "\n",
    "class JobOfferSaverAgent(BaseAgent):\n",
    "    \"\"\"This agent saves the job offer details into the session state.\"\"\"\n",
    "\n",
    "    @t.override\n",
    "    async def _run_async_impl(self, ctx: InvocationContext) -> t.AsyncGenerator[Event]:\n",
    "        # Get the first event from the session\n",
    "        first_event = ctx.session.events[0]\n",
    "\n",
    "        # If the first event is not from the user, we ignore it\n",
    "        if first_event.author != \"user\":\n",
    "            return\n",
    "\n",
    "        # Otherwise, we try to extract the job offer from the first event\n",
    "        job_offer = None\n",
    "        if (content := first_event.content) and (parts := content.parts):\n",
    "            part = parts[0]\n",
    "            if job_offer := part.text:\n",
    "                # Save the job offer in the session state\n",
    "                ctx.session.state[\"job_offer\"] = job_offer\n",
    "\n",
    "        # Finally, we yield the first event back to the session\n",
    "        yield Event(author=self.name)\n",
    "\n",
    "\n",
    "job_offer_saver_agent = JobOfferSaverAgent(name=\"JobOfferSaverAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Job Summarizer\n",
    "\n",
    "Since the first input is a job offer, a model is needed to summarize the job offer in order to extract the key points. This will help to avoid spending too many tokens when delegating the task of improving the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class JobOfferSummarized(BaseModel):\n",
    "    job_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Concise description of the job\",\n",
    "    )\n",
    "    requirements: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Key requirements listed clearly\",\n",
    "    )\n",
    "    tech_stack: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Technologies and tools mentioned\",\n",
    "    )\n",
    "\n",
    "    __EXAMPLE__ = {\n",
    "        \"job_description\": \"Develop and maintain web applications.\",\n",
    "        \"requirements\": [\"Python\", \"Django\", \"REST APIs\"],\n",
    "        \"tech_stack\": [\"AWS\", \"Docker\", \"PostgreSQL\"],\n",
    "    }\n",
    "\n",
    "\n",
    "assert JobOfferSummarized(  # noqa: S101\n",
    "    **JobOfferSummarized.__EXAMPLE__\n",
    "), \"Example does not conform to schema\"\n",
    "\n",
    "job_offer_summ = JobOfferSummarized(**JobOfferSummarized.__EXAMPLE__)\n",
    "\n",
    "job_offer_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.models.google_llm import Gemini\n",
    "from google.genai import types\n",
    "\n",
    "PROMPT = f\"\"\"\n",
    "You are a Job Offer Summarizer. Your only task is to read the provided job offer\n",
    "and extract the key information such as the description, requirements, stack\n",
    "and provide a concise summary. Do not add any additional information\n",
    "(such as benefits, the title, the company...) or opinions. Try to summarize\n",
    "in 300 words maximum.\n",
    "\n",
    "The output must be only the summarized job offer without any additional\n",
    "commentary.\n",
    "\n",
    "Try to be as specific as possible when extracting the tech stack.\n",
    "\n",
    "Here is an example of the output format:\n",
    "---\n",
    "{json.dumps(JobOfferSummarized.__EXAMPLE__, indent=2)}\n",
    "---\n",
    "\n",
    "You MUST RETURN the output in the EXACT FORMAT as shown above, without any additional text.\n",
    "\"\"\"\n",
    "\n",
    "job_offer_summarizer_agent = Agent(\n",
    "    name=\"JobOfferSummarizerAgent\",\n",
    "    model=config.job_offer_summarizer_model,\n",
    "    description=(\n",
    "        \"An agent that summarizes job offers by extracting key information and\"\n",
    "        \" presenting it clearly to assist job seekers in understanding\"\n",
    "        \" the opportunities.\"\n",
    "    ),\n",
    "    instruction=PROMPT,\n",
    "    output_key=\"summarized_job_offer\",\n",
    "    output_schema=JobOfferSummarized,\n",
    ")\n",
    "\n",
    "print(\"✅ Job Offer Summarizer Agent defined.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d302b",
   "metadata": {},
   "source": [
    "## Experience Enhancement Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d58ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived_exps[\"retrieved_experiences\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = {\n",
    "    \"work\": {\n",
    "        \"summary\": \"Description…\",\n",
    "        \"highlights\": [\"Started the company\"],\n",
    "        \"keywords\": [\"leadership\", \"entrepreneurship\"],\n",
    "    },\n",
    "    \"volunteer\": {\n",
    "        \"summary\": \"Description…\",\n",
    "        \"highlights\": [\"Volunteered at local shelter\"],\n",
    "        \"keywords\": [\"community\", \"helping others\"],\n",
    "    },\n",
    "    \"certificates\": {\n",
    "        \"summary\": \"Description…\",\n",
    "        \"keywords\": [\"certification\", \"achievement\"],\n",
    "    },\n",
    "    \"projects\": {\n",
    "        \"summary\": \"Description…\",\n",
    "        \"highlights\": [\"Developed a web app\"],\n",
    "        \"keywords\": [\"Python\", \"Django\", \"AWS\"],\n",
    "    },\n",
    "    \"skills\": {\n",
    "        \"keywords\": [\"Python\", \"Machine Learning\", \"Data Analysis\"],\n",
    "    },\n",
    "    \"interests\": {\n",
    "        \"keywords\": [\"hiking\", \"photography\", \"travel\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_output_example(exp_kind: str) -> dict[str, str]:\n",
    "    \"\"\"Get an example output for a given experience kind.\n",
    "\n",
    "    Args:\n",
    "        exp_kind (str): A type of experience (e.g., 'work', 'project', 'certificate').\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the status and either an example output or an error message.\n",
    "    \"\"\"\n",
    "    if exp_kind not in EXAMPLES:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": (\n",
    "                f\"Unknown experience kind: {exp_kind}.\"\n",
    "                f\" Plese use one of {list(EXAMPLES.keys())}\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"example\": json.dumps(EXAMPLES[exp_kind], indent=2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8da6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_INITIAL_EXP_ENHANCEMENT = \"\"\"\n",
    "You are an experience enhancer. Your task is to improve the provided\n",
    "description of experience to better align it with the job posting summary.\n",
    "\n",
    "The main goal is to ensure that the experience description highlights\n",
    "the skills, technologies, and accomplishments that are most relevant to\n",
    "the job requirements and technology stack mentioned in the job posting.\n",
    "\n",
    "Use the tool `get_output_example(exp_kind: str)` to get an example output\n",
    "for the kind of experience you are enhancing.\n",
    "\n",
    "YOU MUST RETURN the output in the EXACT FORMAT as shown in the example,\n",
    "without any additional text. NOT FOLLOWING THE FORMAT WILL CAUSE ERRORS.\n",
    "\n",
    "Job Posting Summary:\n",
    "{{summarized_job_offer}}\n",
    "\n",
    "Experience Description:\n",
    "{{experience_{agent_id}}}\n",
    "\n",
    "OUTPUT RULES:\n",
    "- The summary/description must be concise and focused on relevant skills\n",
    "    and accomplishments. Avoid unnecessary details. Try to keep it\n",
    "    in 1-2 sentences.\n",
    "- The highlights must be specific achievements or contributions,\n",
    "    quantifiable where possible. Try to include 2-3 highlights. Each highlight\n",
    "    should start with a strong action verb. Try to keep it in a sentence each.\n",
    "- The keywords must include relevant technologies, tools, and skills.\n",
    "    Try to include 3-5 keywords.\n",
    "- Do not invent details.. Only use the information provided in the\n",
    "    experience description.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_CRITIC = \"\"\"\n",
    "You are a constructive critic. Your task is to review the provided\n",
    "draft of experience and provide feedback on how well it aligns with the\n",
    "job posting summary. Identify areas of improvement, suggest enhancements,\n",
    "and highlight any discrepancies between the draft and the job requirements.\n",
    "\n",
    "Job Posting Summary:\n",
    "{{summarized_job_offer}}\n",
    "\n",
    "Experience Draft:\n",
    "{{enhanced_experience_{agent_id}}}\n",
    "\n",
    "- If the draft is well-aligned, you MUST respond with the exact phrase: \"APPROVED\"\n",
    "- Otherwise, provide 1-3 specific, actionable suggestions for improvement,\n",
    "    of the draft to better align it with the job posting summary.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_REFINER_EXP_ENHANCEMENT = \"\"\"\n",
    "You are an experience enhancer. Your task is to improve the provided\n",
    "description of experience to better align it with the job posting summary.\n",
    "The main goal is to ensure that the experience description highlights\n",
    "the skills, technologies, and accomplishments that are most relevant to\n",
    "the job requirements and technology stack mentioned in the job posting.\n",
    "\n",
    "Use the tool `get_output_example(exp_kind: str)` to get an example output\n",
    "for the kind of experience you are enhancing.\n",
    "\n",
    "YOU MUST RETURN the output in the EXACT FORMAT as shown in the example,\n",
    "without any additional text. NOT FOLLOWING THE FORMAT WILL CAUSE ERRORS.\n",
    "\n",
    "Job Posting Summary:\n",
    "{{summarized_job_offer}}\n",
    "\n",
    "Original Experience Description:\n",
    "{{experience_{agent_id}}}\n",
    "\n",
    "Current Experience Draft:\n",
    "{{enhanced_experience_{agent_id}}}\n",
    "\n",
    "Critique Feedback:\n",
    "{{critique_{agent_id}}}\n",
    "\n",
    "- IF THE CRITIQUE IS \"APPROVED\", you MUST call the `exit_loop` function and nothing else.\n",
    "- OTHERWISE, use the critique feedback to make specific improvements\n",
    "    to the experience draft to better align it with the job posting summary.\n",
    "\n",
    "OUTPUT RULES:\n",
    "- The summary/description must be concise and focused on relevant skills\n",
    "    and accomplishments. Avoid unnecessary details. Try to keep it\n",
    "    in 1-2 sentences.\n",
    "- The highlights must be specific achievements or contributions,\n",
    "    quantifiable where possible. Try to include 2-3 highlights. Each highlight\n",
    "    should start with a strong action verb. Try to keep it in a sentence each.\n",
    "- The keywords must include relevant technologies, tools, and skills.\n",
    "    Try to include 3-5 keywords.\n",
    "- Do not invent details.. Only use the information provided in the\n",
    "    experience description.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb154b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_loop() -> dict[str, t.Any]:\n",
    "    \"\"\"\n",
    "    Call this function ONLY when the critique is 'APPROVED',\n",
    "    indicating the experience is finished and no more changes are needed.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"approved\",\n",
    "        \"message\": \"Experience approved. Exiting refinement loop.\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from google.adk.agents import LoopAgent, SequentialAgent\n",
    "from google.adk.tools import FunctionTool\n",
    "\n",
    "\n",
    "def create_experience_enhancement_agent(\n",
    "    agent_id: str | None = None, max_iterations: int = 1\n",
    ") -> SequentialAgent:\n",
    "    # Use a unique agent ID if not provided\n",
    "    if agent_id is None:\n",
    "        agent_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "    initial_experience_enhancement_agent = Agent(\n",
    "        name=\"InitialExperienceEnhancementAgent\" + agent_id,\n",
    "        model=config.experience_enhancement_model,\n",
    "        description=(\n",
    "            \"An agent that enhances a first draft of experience\"\n",
    "            \" descriptions to better align them with job requirements and\"\n",
    "            \" technology stack.\"\n",
    "        ),\n",
    "        instruction=PROMPT_INITIAL_EXP_ENHANCEMENT.format(agent_id=agent_id),\n",
    "        output_key=\"enhanced_experience_\" + agent_id,\n",
    "        tools=[FunctionTool(get_output_example)],\n",
    "    )\n",
    "\n",
    "    critic_agent = Agent(\n",
    "        name=\"CriticAgent\" + agent_id,\n",
    "        model=config.critic_model,\n",
    "        description=(\n",
    "            \"An agent that critiques and provides feedback on a draft of experience.\"\n",
    "        ),\n",
    "        instruction=PROMPT_CRITIC.format(agent_id=agent_id),\n",
    "        output_key=\"critique_\" + agent_id,\n",
    "    )\n",
    "\n",
    "    refiner_experience_enhancement_agent = Agent(\n",
    "        name=\"RefinerExperienceEnhancementAgent\" + agent_id,\n",
    "        model=config.experience_enhancement_model,\n",
    "        description=(\n",
    "            \"An agent that enhances a draft of experience descriptions based on\"\n",
    "            \" feedback from a critic agent to better align them with job\"\n",
    "            \" requirements and technology stack.\"\n",
    "        ),\n",
    "        instruction=PROMPT_REFINER_EXP_ENHANCEMENT.format(agent_id=agent_id),\n",
    "        output_key=\"enhanced_experience_\" + agent_id,\n",
    "        tools=[FunctionTool(exit_loop), FunctionTool(get_output_example)],\n",
    "    )\n",
    "\n",
    "    experience_refinement_loop = LoopAgent(\n",
    "        name=\"ExperienceRefinementLoopAgent\" + agent_id,\n",
    "        sub_agents=[\n",
    "            critic_agent,\n",
    "            refiner_experience_enhancement_agent,\n",
    "        ],\n",
    "        max_iterations=max_iterations,\n",
    "    )\n",
    "\n",
    "    experience_enhancement_sequence = SequentialAgent(\n",
    "        name=\"ExperienceEnhancementAgent\" + agent_id,\n",
    "        sub_agents=[\n",
    "            initial_experience_enhancement_agent,\n",
    "            experience_refinement_loop,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return experience_enhancement_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceLimits(t.TypedDict, total=False):\n",
    "    work: int\n",
    "    volunteer: int\n",
    "    certificates: int\n",
    "    projects: int\n",
    "    skills: int\n",
    "    interests: int\n",
    "\n",
    "\n",
    "ExperienceLimitsKeys = t.Literal[\n",
    "    \"work\",\n",
    "    \"volunteer\",\n",
    "    \"certificates\",\n",
    "    \"projects\",\n",
    "    \"skills\",\n",
    "    \"interests\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived_exps = retrive_experiences_by_query(\n",
    "    query=\"Python, SQL, Machine Learning\",\n",
    "    n_results=0,\n",
    ")[\"retrieved_experiences\"]\n",
    "\n",
    "\n",
    "def limit_experiences_by_type(\n",
    "    experiences: list[ExperienceRetrieval],\n",
    "    limits: ExperienceLimits,\n",
    ") -> list[ExperienceRetrieval]:\n",
    "    \"\"\"Limit the number of experiences by type.\n",
    "\n",
    "    Args:\n",
    "        experiences (list[ExperienceRetrieval]): The list of experiences to limit.\n",
    "        limits (ExperienceLimits): The limits for each experience type.\n",
    "\n",
    "    Returns:\n",
    "        list[ExperienceRetrieval]: The limited list of experiences.\n",
    "    \"\"\"\n",
    "    from collections import Counter  # noqa: PLC0415\n",
    "\n",
    "    counts: Counter[str] = Counter()\n",
    "    limited_experiences: list[ExperienceRetrieval] = []\n",
    "\n",
    "    for exp in experiences:\n",
    "        exp_type: ExperienceLimitsKeys = exp[\"exp_type\"]  # type: ignore\n",
    "        if exp_type in limits and counts[exp_type] < limits[exp_type]:\n",
    "            limited_experiences.append(exp)\n",
    "            counts.update([exp_type])\n",
    "\n",
    "    return limited_experiences\n",
    "\n",
    "\n",
    "limit_experiences_by_type(\n",
    "    retrived_exps,\n",
    "    limits={\n",
    "        \"work\": 2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from google.adk.agents import ParallelAgent\n",
    "\n",
    "\n",
    "class CVEnhancementAgent(BaseAgent):\n",
    "    \"\"\"This agent enhances a CV based on the job offer summary and\n",
    "    retrieved experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    experience_limits: ExperienceLimits | None = None\n",
    "    max_refinement_iterations: int = 1\n",
    "\n",
    "    @t.override\n",
    "    async def _run_async_impl(self, ctx: InvocationContext) -> t.AsyncGenerator[Event]:\n",
    "        logging.info(f\"[{self.name}] Starting CV Enhancement Agent.\")\n",
    "\n",
    "        # Get the job offer summary from the session state\n",
    "        job_offer_summary = ctx.session.state.get(\"summarized_job_offer\", None)\n",
    "        if job_offer_summary is None:\n",
    "            logging.error(f\"[{self.name}] No job offer summary found in session state.\")\n",
    "            return\n",
    "\n",
    "        # Retrieve the experiences using the retrive_experiences_by_query tool\n",
    "        #  from the session state\n",
    "        retrived_exps = retrive_experiences_by_query(\n",
    "            query=\", \".join(job_offer_summary[\"tech_stack\"]),\n",
    "            n_results=0,\n",
    "        )[\"retrieved_experiences\"]\n",
    "\n",
    "        # Limit the number of experiences if limits are provided\n",
    "        if self.experience_limits:\n",
    "            retrived_exps = limit_experiences_by_type(\n",
    "                retrived_exps,\n",
    "                limits=self.experience_limits,\n",
    "            )\n",
    "        logging.info(\n",
    "            f\"[{self.name}] Retrieved {len(retrived_exps)} experiences for enhancement.\"\n",
    "        )\n",
    "        logging.debug(f\"[{self.name}] Experiences: {retrived_exps}\")\n",
    "\n",
    "        # Tag each experience with a unique ID.\n",
    "        #  Also save in the state the experience descriptions\n",
    "        tagged_experiences: dict[str, ExperienceRetrieval] = {}\n",
    "        for exp in retrived_exps:\n",
    "            tag_id = \"_\" + exp[\"exp_id\"].replace(\".\", \"_\")\n",
    "            tagged_experiences[tag_id] = exp\n",
    "            ctx.session.state[f\"experience_{tag_id}\"] = exp[\"description\"]\n",
    "\n",
    "        # Create a list of experience enhancement agents for parallel execution\n",
    "        enhancement_agents = [\n",
    "            create_experience_enhancement_agent(\n",
    "                agent_id=agent_id, max_iterations=self.max_refinement_iterations\n",
    "            )\n",
    "            for agent_id in tagged_experiences\n",
    "        ]\n",
    "\n",
    "        parallel_enhancement_agent = ParallelAgent(\n",
    "            name=\"ParallelExperienceEnhancementAgent\",\n",
    "            sub_agents=enhancement_agents,\n",
    "        )\n",
    "\n",
    "        # Run the parallel enhancement agent\n",
    "        async for event in parallel_enhancement_agent.run_async(ctx):\n",
    "            yield event\n",
    "\n",
    "        # Parse the enhanced experience from the session state\n",
    "        parsed_enhanced_experiences: dict[str, dict[str, t.Any]] = {}\n",
    "        for agent_id in tagged_experiences:\n",
    "            enhanced_experience: str | None = ctx.session.state.get(\n",
    "                f\"enhanced_experience_{agent_id}\"\n",
    "            )\n",
    "            if enhanced_experience:\n",
    "                enhanced_exp = enhanced_experience.replace(\"```json\", \"\").replace(\n",
    "                    \"```\", \"\"\n",
    "                )\n",
    "                parsed_enhanced_experiences[agent_id] = json.loads(enhanced_exp)\n",
    "\n",
    "        logging.info(f\"[{self.name}] Parsed enhanced experiences.\")\n",
    "        logging.debug(\n",
    "            f\"[{self.name}] Enhanced Experiences: {parsed_enhanced_experiences}\"\n",
    "        )\n",
    "\n",
    "        # Build a json resume with the enhanced experiences\n",
    "        json_resume_original = get_json_resume()\n",
    "        json_resume_enhanced: dict[str, dict | list] = {}\n",
    "        if basics := json_resume_original.basics:\n",
    "            json_resume_enhanced[\"basics\"] = basics.model_dump()\n",
    "\n",
    "        for agent_id, enhanced_exp in parsed_enhanced_experiences.items():\n",
    "            exp_id = tagged_experiences[agent_id][\"exp_id\"]\n",
    "            exp_type = tagged_experiences[agent_id][\"exp_type\"]\n",
    "            if exp_type not in json_resume_enhanced:\n",
    "                json_resume_enhanced[exp_type] = []\n",
    "            experience_obj = find_experience_from_id(exp_id).model_dump()  # type: ignore\n",
    "            if experience_obj:\n",
    "                for field, value in enhanced_exp.items():\n",
    "                    experience_obj[field] = value\n",
    "                json_resume_enhanced[exp_type].append(experience_obj)  # type: ignore\n",
    "\n",
    "        logging.info(f\"[{self.name}] CV Enhancement Agent completed.\")\n",
    "        yield Event(\n",
    "            author=self.name,\n",
    "            content={\n",
    "                \"parts\": [{\n",
    "                    \"text\": JsonResume(**json_resume_enhanced).model_dump_json(indent=2)\n",
    "                }]\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "cv_enhancement_agent = CVEnhancementAgent(\n",
    "    name=\"CVEnhancementAgent\",\n",
    "    experience_limits=ExperienceLimits(\n",
    "        work=3,\n",
    "        projects=1,\n",
    "        certificates=1,\n",
    "        skills=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import SequentialAgent\n",
    "from google.adk.runners import InMemoryRunner\n",
    "\n",
    "root_agent = SequentialAgent(\n",
    "    name=\"CvEnhancerRootAgent\",\n",
    "    sub_agents=[\n",
    "        job_offer_saver_agent,\n",
    "        job_offer_summarizer_agent,\n",
    "        cv_enhancement_agent,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.plugins.logging_plugin import LoggingPlugin\n",
    "\n",
    "runner = InMemoryRunner(\n",
    "    agent=root_agent,\n",
    "    plugins=[\n",
    "        LoggingPlugin(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"✅ Runner created.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offer obtained from: https://www.linkedin.com/jobs/collections/?currentJobId=4324487265\n",
    "\n",
    "response = await runner.run_debug(\"\"\"\n",
    "We are still looking for talent… and we would love for you to join our team!\n",
    "\n",
    "For over 25 years, UST has worked alongside the world's best companies to make a real impact through business transformation. Driven by technology, inspired by people, and guided by our purpose, UST supports clients from design to implementation. Together, with more than 30,000 employees in 30 countries, we build to create limitless impact, reaching billions of lives in the process.\n",
    "\n",
    "\n",
    "We are looking for an AI Engineer, to join a strategic project supporting data platform modernization.\n",
    "\n",
    "\n",
    "\n",
    "UST is looking for a candidate with strong Python expertise, proven experience building applications using LLMs, and hands-on exposure to agentic AI frameworks.\n",
    "\n",
    "\n",
    "\n",
    "What We're Looking For:\n",
    "\n",
    "Experience: 6 to 8 years of professional experience.\n",
    "Language: Advanced English B2 - C1\n",
    "Strong programming expertise in Python.\n",
    "Proven experience building applications using any LLMs.\n",
    "1 to 2 years of hands-on experience with agentic AI frameworks.\n",
    "Familiarity with cloud platforms.\n",
    "\n",
    "\n",
    "Why Join Us:\n",
    "\n",
    "Work in a remote and global environment\n",
    "Be part of a mission-critical application team\n",
    "Work in a supportive, collaborative environment\n",
    "Opportunity to mentor others and influence product direction\n",
    "Gain exposure to cloud migration and modern development practices\n",
    "\n",
    "\n",
    "UST is waiting for you!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_response = response[-1]\n",
    "\n",
    "enhanced_cv = None\n",
    "if (content := last_response.content) and (parts := content.parts):\n",
    "    part = parts[0]\n",
    "    if cv_json := part.text:\n",
    "        enhanced_cv = JsonResume.model_validate_json(cv_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fee2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(enhanced_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-enhancer-gen-ai (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
