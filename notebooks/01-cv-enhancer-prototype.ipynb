{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455c403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1857d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964737f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.schemas import JsonResume  # type: ignore\n",
    "\n",
    "type FileType = str | Path | None\n",
    "\n",
    "DEFAULT_FILE = \"../db/json_resume_example.json\"\n",
    "\n",
    "\n",
    "def get_json_resume(file: FileType = None) -> JsonResume:\n",
    "    \"\"\"Load a JSON resume from a file and validate it against the JsonResume schema.\n",
    "\n",
    "    Args:\n",
    "        file (str | Path): The path to the JSON resume file.\n",
    "\n",
    "    Returns:\n",
    "        JsonResume: The validated JSON resume object.\n",
    "    \"\"\"\n",
    "    if file is None:\n",
    "        file = DEFAULT_FILE\n",
    "\n",
    "    with Path(file).open() as json_resume_path:\n",
    "        json_resume = JsonResume.model_validate_json(json_resume_path.read())\n",
    "\n",
    "    return json_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "json_resume = get_json_resume()\n",
    "\n",
    "rprint(json_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ca5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_bfs(tree: dict):\n",
    "    \"\"\"Iterate through a nested dictionary using BFS (Breadth-First Search).\n",
    "\n",
    "    Args:\n",
    "        tree (dict): The nested dictionary to traverse.\n",
    "\n",
    "    Yields:\n",
    "        tuple[str, dict]: A tuple containing the key and dictionary node for\n",
    "            each dictionary found in the tree structure.\n",
    "    \"\"\"\n",
    "    from collections import deque  # noqa: PLC0415\n",
    "\n",
    "    queue = deque([(\"root\", tree)])\n",
    "\n",
    "    while queue:\n",
    "        key, current_node = queue.pop()\n",
    "\n",
    "        # Skip non-dict and non-list nodes\n",
    "        if isinstance(current_node, dict):\n",
    "            yield key, current_node\n",
    "\n",
    "        # If the current node is a dictionary, we add its values to the queue\n",
    "        #  (they are the child nodes)\n",
    "        if isinstance(current_node, dict):\n",
    "            for child_key, child_value in current_node.items():\n",
    "                if isinstance(child_value, (dict, list)):\n",
    "                    queue.appendleft((child_key, child_value))\n",
    "\n",
    "        # The node can also be a list. We need to handle that case too.\n",
    "        if isinstance(current_node, list):\n",
    "            for item in current_node:\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    queue.appendleft((key, item))\n",
    "\n",
    "\n",
    "for key, node in iterate_bfs(json_resume.model_dump()):\n",
    "    rprint(f\"[bright_blue]{key}[/bright_blue]: {repr(node)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99101cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks_by_experience(file: FileType = None) -> dict[str, list[str]]:\n",
    "    \"\"\"Get technology stacks organized by experience type from the JSON resume.\n",
    "\n",
    "    This function traverses the JSON resume structure using BFS (Breadth-First Search)\n",
    "    to find all \"keywords\" fields and organizes them by their parent context\n",
    "    (e.g., work experience, skills, projects, etc.).\n",
    "\n",
    "    Args:\n",
    "        file (FileType, optional): The path to the JSON resume file. Defaults\n",
    "            to the principal JSON resume file.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: A dictionary where keys represent the experience\n",
    "            type or context (e.g., 'work', 'skills', 'projects') and values\n",
    "            are sorted lists of unique technology stacks/keywords found in\n",
    "            that context.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict  # noqa: PLC0415\n",
    "\n",
    "    json_resume = get_json_resume(file)\n",
    "\n",
    "    # Convert the Pydantic model to a dictionary\n",
    "    dict_resume = json_resume.model_dump()\n",
    "\n",
    "    # Iterate through the JSON structure using BFS\n",
    "    stacks = defaultdict(list)\n",
    "\n",
    "    for name, experience_node in iterate_bfs(dict_resume):\n",
    "        if keywords := experience_node.get(\"keywords\"):\n",
    "            # Sometimes keywords can be None. We skip those cases.\n",
    "            if keywords is None:\n",
    "                continue\n",
    "            stacks[name].extend(keywords)\n",
    "\n",
    "    # Remove duplicates and sort the stacks\n",
    "    stacks = {key: sorted(set(value)) for key, value in stacks.items()}\n",
    "\n",
    "    return stacks\n",
    "\n",
    "\n",
    "def get_all_stacks(file: FileType = None) -> dict[str, list[str]]:\n",
    "    \"\"\"Get a sorted list of all unique technology stacks/keywords from the JSON resume.\n",
    "\n",
    "    Args:\n",
    "        file (FileType, optional): The path to the JSON resume file. Defaults\n",
    "            to the principal JSON resume file.\n",
    "    Returns:\n",
    "        list[str]: A sorted list of all unique technology stacks/keywords.\n",
    "    \"\"\"\n",
    "    stacks_by_experience = get_stacks_by_experience(file)\n",
    "    all_stacks = set()\n",
    "    for stacks in stacks_by_experience.values():\n",
    "        all_stacks.update(stacks)\n",
    "    return {\"stacks\": sorted(all_stacks)}\n",
    "\n",
    "\n",
    "def get_experience_by_stacks(\n",
    "    stacks: list[str], file: FileType = None\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Get experience types/contexts for given technology stacks/keywords.\n",
    "\n",
    "    Args:\n",
    "        stacks (list[str]): A list of technology stacks/keywords to search for.\n",
    "        file (FileType, optional): The path to the JSON resume file. Defaults\n",
    "            to the principal JSON resume file.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: A dictionary where keys are the provided stacks\n",
    "            and values are lists of experience types/contexts where each stack\n",
    "            is found.\n",
    "    \"\"\"\n",
    "    stacks_set = set(stacks)\n",
    "\n",
    "    experiences_found = []\n",
    "    json_resume = get_json_resume(file)\n",
    "    for name, experience_node in iterate_bfs(json_resume.model_dump()):\n",
    "        if keywords := experience_node.get(\"keywords\"):\n",
    "            # Sometimes keywords can be None. We skip those cases.\n",
    "            if keywords is None:\n",
    "                continue\n",
    "            intersection = stacks_set.intersection(set(keywords))\n",
    "            if intersection:\n",
    "                rprint(\n",
    "                    f\"[green]Found stacks {intersection} in context:[/green] \"\n",
    "                    f\"{repr(experience_node)[:100]}\"\n",
    "                )\n",
    "                experiences_found.append((name, experience_node))\n",
    "\n",
    "    return experiences_found\n",
    "\n",
    "\n",
    "get_experience_by_stacks([\"Python\", \"Django\", \"AWS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stacks_by_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aeb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_stacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431f2ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "351bb3c4",
   "metadata": {},
   "source": [
    "# CV Enhancer\n",
    "\n",
    "El objetivo es crear un agente que ayude a mejorar un currículum vitae (CV) agregando y enfocando las experiencias laborales y habilidades relevantes para un puesto de trabajo específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.models.google_llm import Gemini\n",
    "from google.genai import types\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "\n",
    "class CvSaverConfig:\n",
    "\n",
    "    job_offer_summarizer_model: Gemini = Gemini(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        retry_options=retry_config,\n",
    "    )\n",
    "\n",
    "    writer_model: Gemini = Gemini(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        retry_options=retry_config,\n",
    "    )\n",
    "\n",
    "\n",
    "config = CvSaverConfig()\n",
    "\n",
    "config.job_offer_summarizer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5719d",
   "metadata": {},
   "source": [
    "## Job Summarizer\n",
    "\n",
    "Since the first input is a job offer, a model is needed to summarize the job offer in order to extract the key points. This will help to avoid spending too many tokens when delegating the task of improving the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class JobOfferSummarized(BaseModel):\n",
    "    job_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Concise description of the job\",\n",
    "    )\n",
    "    requirements: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Key requirements listed clearly\",\n",
    "    )\n",
    "    tech_stack: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Technologies and tools mentioned\",\n",
    "    )\n",
    "\n",
    "    __EXAMPLE__ = {\n",
    "        \"job_description\": \"Develop and maintain web applications.\",\n",
    "        \"requirements\": [\"Python\", \"Django\", \"REST APIs\"],\n",
    "        \"tech_stack\": [\"AWS\", \"Docker\", \"PostgreSQL\"],\n",
    "    }\n",
    "\n",
    "\n",
    "assert JobOfferSummarized(  # noqa: S101\n",
    "    **JobOfferSummarized.__EXAMPLE__\n",
    "), \"Example does not conform to schema\"\n",
    "\n",
    "job_offer_summ = JobOfferSummarized(**JobOfferSummarized.__EXAMPLE__)\n",
    "\n",
    "job_offer_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7872ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.genai import types\n",
    "\n",
    "PROMPT = f\"\"\"\n",
    "You are a Job Offer Summarizer. Your only task is to read the provided job offer\n",
    "and extract the key information such as the description, requirements, stack\n",
    "and provide a concise summary. Do not add any additional information\n",
    "(such as benefits, the title, the company...) or opinions.\n",
    "\n",
    "The output must be only the summarized job offer without any additional\n",
    "commentary.\n",
    "\n",
    "Here is an example of the output format:\n",
    "---\n",
    "{json.dumps(JobOfferSummarized.__EXAMPLE__, indent=2)}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "job_offer_summarizer_agent = Agent(\n",
    "    name=\"JobOfferSummarizerAgent\",\n",
    "    model=config.job_offer_summarizer_model,\n",
    "    description=(\n",
    "        \"An agent that summarizes job offers by extracting key information and\"\n",
    "        \" presenting it clearly to assist job seekers in understanding\"\n",
    "        \" the opportunities.\"\n",
    "    ),\n",
    "    instruction=PROMPT,\n",
    "    output_key=\"summarized_job_offer\",\n",
    "    output_schema=JobOfferSummarized,\n",
    ")\n",
    "\n",
    "# Expose the agent for external use\n",
    "root_agent = job_offer_summarizer_agent\n",
    "\n",
    "print(\"✅ Root Agent defined.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bae36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.runners import InMemoryRunner\n",
    "\n",
    "runner = InMemoryRunner(agent=root_agent)\n",
    "\n",
    "print(\"✅ Runner created.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await runner.run_debug(\"\"\"\n",
    "NeuralWorks es una compañía de alto crecimiento fundada hace aproximadamente 4 años. Estamos trabajando a toda máquina en cosas que darán que hablar.\n",
    "\n",
    "Somos un equipo donde se unen la creatividad, curiosidad y la pasión por hacer las cosas bien. Nos arriesgamos a explorar fronteras donde otros no llegan: un modelo predictor basado en monte carlo, una red convolucional para detección de caras, un sensor de posición bluetooth, la recreación de un espacio acústico usando finite impulse response.\n",
    "\n",
    "Estos son solo algunos de los desafíos, donde aprendemos, exploramos y nos complementamos como equipo para lograr cosas impensadas.\n",
    "\n",
    "Trabajamos en proyectos propios y apoyamos a corporaciones en partnerships donde codo a codo combinamos conocimiento con creatividad, donde imaginamos, diseñamos y creamos productos digitales capaces de cautivar y crear impacto.\n",
    "\n",
    " Descripción del trabajo\n",
    "\n",
    "El equipo de Analytics trabaja en diferentes proyectos que combinan volúmenes de datos enormes e IA, como detectar y predecir fallas antes que ocurran, optimizar pricing, personalizar la experiencia del cliente, optimizar uso de combustible, detectar caras y objetos usando visión por computador.\n",
    "\n",
    "Como Data Scientist, trabajarás en conjunto con Machine Learning Engineers,Translators, Data Engineers entre otros perfiles, construyendo productos basados en datos que permitan llevar las prácticas de negocio al siguiente nivel.\n",
    "\n",
    "El equipo de Analítica no opera como un equipo de Research separado, sino que está profundamente integrado con nuestros clientes y sus unidades de negocio, esto permite un feedback loop rápido para iterar, corregir y perfeccionar para finalmente conseguir el impacto de negocio.\n",
    "\n",
    "En cualquier proyecto que trabajes, esperamos que tengas un gran espíritu de colaboración, una pasión por la innovación y el código y una mentalidad de automatización antes que procesos manuales.\n",
    "\n",
    "Tu trabajo variará de proyecto a proyecto, pero siguiendo una estructura similar:\n",
    "\n",
    "Identificar problemas de analítica que tengan un mayor impacto para la organización.\n",
    "Familiarizarte con el problema junto a expertos, recabar información, opiniones y facts\n",
    "Investigar las mejores prácticas de la industria.\n",
    "Definir hipótesis de trabajo y approach técnico para resolver el problema, incluyendo el set de datos a utilizar, variables, periodo de tiempo, etc.\n",
    "Recopilar datasets estructurados y no estructurados desde múltiples fuentes.\n",
    "Realizar limpieza de datos y validar correctitud.\n",
    "Aplicar modelos y algoritmos para minería de datos.\n",
    "Analizar resultados e identificar patrones y tendencias.\n",
    "Diseñar junto a Machine Learning Engineers soluciones que permitan capturar oportunidades.\n",
    "Comunicar los resultados y discutirlos junto a expertos o tomadores de decisiones.\n",
    "Algunos proyectos pueden incluir desafíos complejos en visión por computador.\n",
    "\n",
    " Calificaciones clave\n",
    "\n",
    "Estudios de Ingeniería Civil en Computación o similar.\n",
    "Experiencia previa en roles de Data Scientist, Data Engineer o similar.\n",
    "Experiencia con Python.\n",
    "Entendimiento de estructuras de datos con habilidades analíticas relacionadas con el trabajo con conjuntos de datos no estructurados, conocimiento avanzado de SQL, incluida la optimización de consultas.\n",
    "Entendimiento de flujo de vida completo modelos de Machine Learning y productos de datos.\n",
    "Habilidades analíticas, de diseño y resolución de problemas.\n",
    "Habilidades de colaboración y comunicación.\n",
    "Buen manejo comunicacional y skills colaborativos.\n",
    "Buen manejo de inglés, sobre todo en lectura donde debes ser capaz de leer un paper, artículos o documentación de forma constante.\n",
    "\n",
    "¡En NeuralWorks nos importa la diversidad! Creemos firmemente en la creación de un ambiente laboral inclusivo, diverso y equitativo. Reconocemos y celebramos la diversidad en todas sus formas y estamos comprometidos a ofrecer igualdad de oportunidades para todos los candidatos.\n",
    "\n",
    "“Los hombres postulan a un cargo cuando cumplen el 60% de las calificaciones, pero las mujeres sólo si cumplen el 100%.” D. Gaucher , J. Friesen and A. C. Kay, Journal of Personality and Social Psychology, 2011.\n",
    "\n",
    "Te invitamos a postular aunque no cumplas con todos los requisitos.\n",
    "\n",
    " Nice to have\n",
    "\n",
    "Experiencia con servidores cloud (GCP, AWS o Azure), especialmente el conjunto de servicios de procesamiento de datos.\n",
    "Experiencia usando pipelines de CI/CD y Docker.\n",
    "Experiencia en visión por computador (Computer Vision), incluyendo uso de librerías como OpenCV, PyTorch/TensorFlow, YOLO, Detectron2 u otras.\n",
    "\n",
    " Beneficios\n",
    "\n",
    "MacBook Air M2 o similar (con opción de compra hiper conveniente)\n",
    "Bono por desempeño\n",
    "Bono de almuerzo mensual y almuerzo de equipo los viernes\n",
    "Seguro Complementario de salud y dental\n",
    "Horario flexible\n",
    "Flexibilidad entre oficina y home office\n",
    "Medio día libre el día de tu cumpleaños\n",
    "Financiamiento de certificaciones\n",
    "Inscripción en Coursera con plan de entrenamiento a medida\n",
    "Estacionamiento de bicicletas\n",
    "Programa de referidos\n",
    "Salida de “teambuilding” mensual\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c46d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb555856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_simiarity(\n",
    "    set1: set[str], set2: set[str], reference_set: set[str] | None = None\n",
    ") -> float:\n",
    "    if reference_set is None:\n",
    "        reference_set = set1.union(set2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(reference_set)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-enhancer-gen-ai (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
